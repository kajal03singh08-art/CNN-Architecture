{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture"
      ],
      "metadata": {
        "id": "msXS56i5FUn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "Answer : Role of Filters and Feature Maps in Convolutional Neural Networks (CNN)\n",
        "\n",
        "In a Convolutional Neural Network (CNN), filters and feature maps are fundamental components used for automatic feature extraction from input data, especially images. Filters (or kernels) are small learnable matrices that move across the input image and perform convolution operations. Their primary role is to detect important local features such as edges, corners, textures, and shapes. Each filter is designed to learn a specific pattern present in the image.\n",
        "\n",
        "When a filter is applied to the input image, it generates an output called a feature map. A feature map represents the spatial distribution of the detected feature, showing where and how strongly a particular feature appears in the image. Higher activation values in the feature map indicate a stronger presence of the feature, while lower values indicate weaker or no activation.\n",
        "\n",
        "By using multiple filters, CNNs generate multiple feature maps that capture different types of features at various levels of complexity. This process enables CNNs to learn hierarchical representations, where early layers detect simple features and deeper layers capture more complex and abstract patterns, improving the performance of tasks such as image classification and object recognition.\n",
        "\n",
        "Question 2: Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Answer : Padding and Stride in Convolutional Neural Networks (CNNs)\n",
        "\n",
        "In Convolutional Neural Networks (CNNs), padding and stride are important hyperparameters that control how convolution operations are applied to the input and how the size of the output feature maps is determined.\n",
        "\n",
        "Padding refers to the addition of extra pixels, usually with zero values, around the border of the input image before applying the convolution filter. The main purpose of padding is to preserve the spatial dimensions of the input and to ensure that edge information is not lost during convolution. Common types of padding include valid padding (no padding) and same padding (padding added to keep the output size the same as the input size).\n",
        "\n",
        "Stride defines the number of pixels by which the filter moves across the input image during the convolution operation. A stride of 1 means the filter moves one pixel at a time, resulting in a larger output feature map. A larger stride value reduces the size of the output feature map by skipping pixels, which also decreases computational complexity.\n",
        "\n",
        "Both padding and stride directly affect the dimensions of the output feature map. Padding increases or preserves the spatial size, while stride controls the rate of downsampling. The output size can be calculated using the formula:\n",
        "\n",
        "Output Size\n",
        "=\n",
        "(\n",
        "ùëÅ\n",
        "‚àí\n",
        "ùêπ\n",
        "+\n",
        "2\n",
        "ùëÉ\n",
        ")\n",
        "ùëÜ\n",
        "+\n",
        "1\n",
        "Output Size=\n",
        "S\n",
        "(N‚àíF+2P)\n",
        "\t‚Äã\n",
        "\n",
        "+1\n",
        "\n",
        "where\n",
        "ùëÅ\n",
        "N is the input size,\n",
        "ùêπ\n",
        "F is the filter size,\n",
        "ùëÉ\n",
        "P is the padding, and\n",
        "ùëÜ\n",
        "S is the stride.\n",
        "\n",
        "Thus, padding and stride play a crucial role in controlling feature map dimensions and preserving important information in CNNs.\n",
        "\n",
        "\n",
        "Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "\n",
        "Answer : Receptive Field in Convolutional Neural Networks (CNNs)\n",
        "\n",
        "In the context of Convolutional Neural Networks (CNNs), the receptive field refers to the specific region of the input image that influences the activation of a particular neuron in a feature map. In other words, it is the area of the input that a neuron ‚Äúsees‚Äù or responds to when producing its output.\n",
        "\n",
        "The receptive field is important for deep CNN architectures because it determines how much contextual information a neuron can capture. In early layers, neurons have small receptive fields and focus on local features such as edges and textures. As the network becomes deeper, the receptive field increases, allowing neurons to capture larger and more complex patterns, such as shapes, objects, or entire regions of an image.\n",
        "\n",
        "A larger receptive field enables deep architectures to learn hierarchical feature representations by combining simple features from earlier layers into more abstract and meaningful features in deeper layers. This capability is essential for tasks like image classification, object detection, and scene understanding, where global context and spatial relationships play a critical role.\n",
        "\n",
        "\n",
        "Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "Answer : Influence of Filter Size and Stride on the Number of Parameters in CNN\n",
        "\n",
        "In a Convolutional Neural Network (CNN), the number of parameters is mainly determined by the filter size, the number of filters, and the depth of the input, while stride affects the output feature map size and computational cost but does not directly change the number of parameters.\n",
        "\n",
        "Filter size directly influences the number of parameters in a CNN. A larger filter contains more weights, which increases the total number of trainable parameters. For example, a\n",
        "3\n",
        "√ó\n",
        "3\n",
        "3√ó3 filter has fewer parameters than a\n",
        "5\n",
        "√ó\n",
        "5\n",
        "5√ó5 filter. The total number of parameters in a convolutional layer is calculated as:\n",
        "\n",
        "(\n",
        "Filter height\n",
        "√ó\n",
        "Filter width\n",
        "√ó\n",
        "Input channels\n",
        "+\n",
        "1\n",
        ")\n",
        "√ó\n",
        "Number of filters\n",
        "(Filter height√óFilter width√óInput channels+1)√óNumber of filters\n",
        "\n",
        "where the extra 1 represents the bias term. Therefore, increasing the filter size increases the model‚Äôs capacity as well as the risk of overfitting.\n",
        "\n",
        "Stride, on the other hand, controls how the filter moves across the input image. Changing the stride does not alter the number of parameters because the same filter weights are reused at every position. However, a larger stride reduces the size of the output feature maps, which lowers the number of activations and reduces computational complexity in subsequent layers.\n",
        "\n",
        "In summary, filter size directly affects the number of parameters, while stride affects the spatial dimensions of feature maps and computational efficiency but not the number of parameters.\n",
        "\n",
        "\n",
        "Question 5: Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "Answer : Comparison of CNN Architectures ‚Äì LeNet, AlexNet, and VGG\n",
        "\n",
        "LeNet, AlexNet, and VGG are landmark CNN architectures that show the evolution of deep learning models in terms of depth, filter sizes, and performance.\n",
        "\n",
        "LeNet is one of the earliest CNN architectures, designed primarily for handwritten digit recognition. It is a shallow network with a small number of layers and uses relatively large filter sizes in the initial layers. LeNet has a low number of parameters and computational requirements, making it suitable for simple tasks, but its performance is limited on complex image datasets.\n",
        "\n",
        "AlexNet marked a major breakthrough in deep learning by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. It is deeper than LeNet and uses multiple convolutional layers with a combination of large and smaller filter sizes. AlexNet introduced important concepts such as ReLU activation, dropout, and GPU-based training, which significantly improved performance on large-scale image classification tasks.\n",
        "\n",
        "VGG further increased network depth by using a very deep architecture with many convolutional layers. Unlike AlexNet, VGG consistently uses small\n",
        "3\n",
        "√ó\n",
        "3\n",
        "3√ó3 filters throughout the network. This design increases the number of layers and parameters, allowing the model to learn more complex and hierarchical features. VGG achieved strong performance on ImageNet but at the cost of high computational and memory requirements.\n",
        "\n",
        "In comparison, LeNet is shallow with limited performance, AlexNet offers improved depth and performance with moderate complexity, and VGG is very deep with superior feature representation but high computational cost.\n",
        "\n",
        "\n",
        "Question 6: Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "Answer :\n",
        "\n",
        "# Import required libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST dataset\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "\n",
        "x_train = x_train.reshape(60000, 28, 28, 1) / 255.0\n",
        "\n",
        "x_test = x_test.reshape(10000, 28, 28, 1) / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the CNN model\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "\n",
        "                    epochs=5,\n",
        "\n",
        "                    batch_size=64,\n",
        "\n",
        "                    validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "- Sample Output:\n",
        "\n",
        "Epoch 1/5\n",
        "\n",
        "843/843 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10s 11ms/step - accuracy: 0.9201 - loss: 0.2604 -\n",
        " val_accuracy: 0.9783 - val_loss: 0.0725\n",
        "Epoch 2/5\n",
        "843/843 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 10ms/step - accuracy: 0.9831 - loss: 0.0546 - val_accuracy: 0.9856 - val_loss: 0.0468\n",
        "Epoch 3/5\n",
        "843/843 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 10ms/step - accuracy: 0.9892 - loss: 0.0332 - val_accuracy: 0.9891 - val_loss: 0.0382\n",
        "Epoch 4/5\n",
        "843/843 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 10ms/step - accuracy: 0.9923 - loss: 0.0234 - val_accuracy: 0.9904 - val_loss: 0.0351\n",
        "Epoch 5/5\n",
        "843/843 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9s 10ms/step - accuracy: 0.9941 - loss: 0.0178 - val_accuracy: 0.9912 - val_loss: 0.0329\n",
        "\n",
        "313/313 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 3ms/step - accuracy: 0.9910 - loss: 0.0307\n",
        "Test Loss: 0.0307\n",
        "Test Accuracy: 0.9910\n",
        "\n",
        "\n",
        "Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "Answer : # Import required libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode class labels\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build CNN model for RGB images\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "\n",
        "                        input_shape=(32, 32, 3)))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display model architecture\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "\n",
        "          epochs=10,\n",
        "\n",
        "          batch_size=64,\n",
        "\n",
        "          validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "Model: \"sequential\"\n",
        "\n",
        "Expected Output (Sample):\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #\n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 30, 30, 32)        896\n",
        "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0\n",
        "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496\n",
        "max_pooling2d_1 (MaxPooling2D) (None, 6, 6, 64)        0\n",
        "conv2d_2 (Conv2D)            (None, 4, 4, 128)         73856\n",
        "max_pooling2d_2 (MaxPooling2D) (None, 2, 2, 128)       0\n",
        "flatten (Flatten)            (None, 512)               0\n",
        "dense (Dense)                (None, 128)               65664\n",
        "dense_1 (Dense)              (None, 10)                1290\n",
        "=================================================================\n",
        "\n",
        "\n",
        "Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "Answer : # Import required libraries\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Device configuration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data preprocessing and loading\n",
        "\n",
        "transform = transforms.Compose([\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data',\n",
        "\n",
        "                               train=True,\n",
        "\n",
        "                               transform=transform,\n",
        "\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data',\n",
        "\n",
        "                              train=False,\n",
        "\n",
        "                              transform=transform,\n",
        "\n",
        "                              download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define CNN model\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.relu(self.conv1(x))\n",
        "\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Model evaluation\n",
        "\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "Sample Output:\n",
        "\n",
        "Epoch [1/5], Loss: 0.1856\n",
        "\n",
        "Epoch [2/5], Loss: 0.0583\n",
        "\n",
        "Epoch [3/5], Loss: 0.0412\n",
        "\n",
        "Epoch [4/5], Loss: 0.0316\n",
        "\n",
        "Epoch [5/5], Loss: 0.0254\n",
        "\n",
        "Test Accuracy: 98.9%\n",
        "\n",
        "\n",
        "Question 9: Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "Answer : # Import required libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        " Define dataset directory structure\n",
        "\n",
        " dataset/\n",
        "\n",
        " ‚îú‚îÄ‚îÄ train/\n",
        "\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ class1/\n",
        "\n",
        " ‚îÇ   ‚îú‚îÄ‚îÄ class2/\n",
        "\n",
        " ‚îî‚îÄ‚îÄ validation/\n",
        "\n",
        "     ‚îú‚îÄ‚îÄ class1/\n",
        "     \n",
        "    ‚îú‚îÄ‚îÄ class2/\n",
        "\n",
        "train_dir = \"dataset/train\"\n",
        "\n",
        "validation_dir = \"dataset/validation\"\n",
        "\n",
        "# ImageDataGenerator for preprocessing and augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "\n",
        "    rescale=1./255,\n",
        "\n",
        "    rotation_range=20,\n",
        "\n",
        "    width_shift_range=0.2,\n",
        "\n",
        "    height_shift_range=0.2,\n",
        "\n",
        "    zoom_range=0.2,\n",
        "\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load images from directory\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "\n",
        "    train_dir,\n",
        "\n",
        "    target_size=(128, 128),\n",
        "\n",
        "    batch_size=32,\n",
        "\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\n",
        "    validation_dir,\n",
        "\n",
        "    target_size=(128, 128),\n",
        "\n",
        "    batch_size=32,\n",
        "\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build CNN model\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "\n",
        "                        input_shape=(128, 128, 3)))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(\n",
        "\n",
        "    optimizer='adam',\n",
        "\n",
        "    loss='categorical_crossentropy',\n",
        "\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "\n",
        "history = model.fit(\n",
        "\n",
        "    train_generator,\n",
        "\n",
        "    epochs=10,\n",
        "\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "\n",
        "print(\"Validation Loss:\", loss)\n",
        "\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "Sample Output:\n",
        "\n",
        "Found 800 images belonging to 2 classes.\n",
        "Found 200 images belonging to 2 classes.\n",
        "Epoch 1/10\n",
        "25/25 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ accuracy: 0.78 - loss: 0.52 - val_accuracy: 0.82 - val_loss: 0.45\n",
        "...\n",
        "Epoch 10/10\n",
        "accuracy: 0.93 - loss: 0.18 - val_accuracy: 0.90 - val_loss: 0.25\n",
        "\n",
        "Validation Loss: 0.25\n",
        "Validation Accuracy: 0.90\n",
        "\n",
        "Question 10: You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into ‚ÄúNormal‚Äùand ‚ÄúPneumonia‚Äù categories. Describe your end-to-end approach‚Äìfrom data preparation and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        "Answer: End-to-End Approach for Chest X-ray Classification Using CNN and Streamlit\n",
        "\n",
        "To build and deploy a CNN model that classifies chest X-ray images into Normal and Pneumonia categories, an end-to-end pipeline is followed, covering data preparation, model training, evaluation, and deployment as a web application.\n",
        "\n",
        "Data Preparation:\n",
        "\n",
        "The first step involves collecting a labeled chest X-ray dataset and organizing it into training, validation, and testing directories. Since X-ray images vary in size and intensity, images are resized to a fixed resolution and normalized to scale pixel values between 0 and 1. Data augmentation techniques such as rotation, flipping, and zooming are applied to improve model generalization and reduce overfitting.\n",
        "\n",
        "Model Training:\n",
        "\n",
        "A Convolutional Neural Network (CNN) is designed or a pre-trained model such as ResNet or VGG is fine-tuned using transfer learning. The model learns discriminative features from X-ray images through convolution, pooling, and fully connected layers. The network is compiled with an appropriate optimizer (e.g., Adam) and a binary classification loss function. Model performance is evaluated using metrics such as accuracy, precision, recall, and confusion matrix analysis.\n",
        "\n",
        "Model Evaluation and Saving:\n",
        "\n",
        "After training, the model is validated on unseen test data to ensure reliable performance. The trained model is then saved in a format such as HDF5 or SavedModel for deployment.\n",
        "\n",
        "Web Application Deployment Using Streamlit:\n",
        "\n",
        "For deployment, a Streamlit-based web application is developed. The saved CNN model is loaded into the app, and users can upload chest X-ray images through the interface. The uploaded image is preprocessed in the same way as the training data and passed to the model for prediction. The app displays the predicted class (Normal or Pneumonia) along with the confidence score.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "This end-to-end approach ensures a seamless workflow from data preparation and CNN training to real-time deployment, enabling healthcare professionals to quickly and efficiently analyze chest X-ray images through a user-friendly web application.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eBFKq7W-Fdql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a33eNwJFQBG"
      },
      "outputs": [],
      "source": []
    }
  ]
}